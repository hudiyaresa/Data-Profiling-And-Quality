{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mentoring Week 2 - Data Profiling and Data Quality** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Data Pipeline with Python and Pyspark - Pacmann AI ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a Data Engineer, we need to understand and assessing the quality of a given dataset containing sales data. This responsibilities include:\n",
    "\n",
    "1. **Data Profiling:** Explore the dataset to gain insights into its structure and attributes.\n",
    "\n",
    "2. **Data Quality Check:** Assess the validity and consistency of the data. Identify any anomalies or missing values.\n",
    "\n",
    "3. **Recommendations:** Based on your findings, provide recommendations for cleaning and improving the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Dataset**\n",
    "\n",
    "* Use Docker Compose to run the container: [repository](https://github.com/Kurikulum-Sekolah-Pacmann/data_pipeline_paccafe)\n",
    "* This dataset provides detailed information about cafe sales, employees, member customers, and inventory tracking history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Output:** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Profiling** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "DB_SOURCE = {\n",
    "    \"dbname\": os.getenv(\"SRC_POSTGRES_DB\"),\n",
    "    \"user\": os.getenv(\"SRC_POSTGRES_USER\"),\n",
    "    \"password\": os.getenv(\"SRC_POSTGRES_PASSWORD\"),\n",
    "    \"host\": os.getenv(\"SRC_POSTGRES_HOST\"),\n",
    "    \"port\": os.getenv(\"SRC_POSTGRES_PORT\")\n",
    "}\n",
    "\n",
    "def db_engine():\n",
    "    \"\"\"Creates a SQLAlchemy engine.\"\"\"\n",
    "    try:\n",
    "        db_url = f\"postgresql://{DB_SOURCE['user']}:{DB_SOURCE['password']}@{DB_SOURCE['host']}:{DB_SOURCE['port']}/{DB_SOURCE['dbname']}\"\n",
    "        engine = create_engine(db_url)\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating engine: {e}\")\n",
    "        return None\n",
    "\n",
    "def list_tables():\n",
    "    \"\"\"Retrieve all table names from the database.\"\"\"\n",
    "    engine = db_engine()\n",
    "    query = \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    return df[\"table_name\"].tolist()\n",
    "\n",
    "def extract_data():\n",
    "    \"\"\"Extract data from all tables into a dictionary of DataFrames.\"\"\"\n",
    "    tables = list_tables()\n",
    "    engine = db_engine()\n",
    "    data = {}\n",
    "    for table in tables:\n",
    "        data[table] = pd.read_sql(f\"SELECT * FROM {table}\", engine)\n",
    "    return data\n",
    "\n",
    "def table_shapes(data):\n",
    "    \"\"\"Return the shape (rows, columns) of each table.\"\"\"\n",
    "    result = {}\n",
    "    for table, df in data.items():\n",
    "        result[table] = df.shape\n",
    "    return result\n",
    "\n",
    "def column_types(data):\n",
    "    \"\"\"Return data types of columns in each table.\"\"\"\n",
    "    result = {}\n",
    "    for table, df in data.items():\n",
    "        result[table] = {}\n",
    "        for col in df.columns:\n",
    "            result[table][col] = str(df[col].dtype)\n",
    "    return result\n",
    "\n",
    "def unique_values(data):\n",
    "    \"\"\"Return unique values of specific records in each table for specific columns.\"\"\"\n",
    "    selected_columns = {\n",
    "        'employees': ['role'],\n",
    "        'orders': ['payment_method'],\n",
    "        'products': ['category'],\n",
    "        'inventory': ['reason']\n",
    "    }\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # Iterate through selected tables and columns\n",
    "    for table, columns in selected_columns.items():\n",
    "        result[table] = {}\n",
    "        if table in data:\n",
    "            for col in columns:\n",
    "                if col in data[table].columns:\n",
    "                    result[table][col] = data[table][col].unique().tolist()\n",
    "                else:\n",
    "                    result[table][col] = []  # If the column doesn't exist in the table, return an empty list\n",
    "        else:\n",
    "            result[table] = {}  # If the table doesn't exist in the data, return an empty dict\n",
    "\n",
    "    return result\n",
    "\n",
    "def profile_report():\n",
    "    \"\"\"Generate a data profiling report and save as JSON.\"\"\"\n",
    "    data = extract_data()\n",
    "    report = {\n",
    "        \"person_in_charge\": \"Reza\",\n",
    "        \"date_profiling\": str(datetime.now()),\n",
    "        \"result\": {}\n",
    "    }\n",
    "\n",
    "    table_shapes_result = table_shapes(data)\n",
    "    column_types_result = column_types(data)\n",
    "    unique_values_result = unique_values(data)\n",
    "\n",
    "    for table in data.keys():\n",
    "        report[\"result\"][table] = {\n",
    "            \"shape\": table_shapes_result[table],\n",
    "            \"data_types\": column_types_result[table],\n",
    "            \"unique_values\": unique_values_result.get(table, {})\n",
    "        }\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    output_folder = 'output'\n",
    "    output_file = 'paccafe_profiling_report.json'\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    output_path = os.path.join(output_folder, output_file)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "\n",
    "    return report\n",
    "\n",
    "# Running profiling\n",
    "profile_report()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Quality** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality report saved to output\\paccafe_quality_report.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'person_in_charge': 'Reza',\n",
       " 'date_quality_check': '2025-02-16 22:09:31.530729',\n",
       " 'result': {'products': {'missing_values': {'product_id': 0,\n",
       "    'product_name': 0,\n",
       "    'category': 0,\n",
       "    'unit_price': 0,\n",
       "    'cost_price': 0,\n",
       "    'in_stock': 0,\n",
       "    'created_at': 0},\n",
       "   'date_validity': {},\n",
       "   'numeric_validity': {'unit_price': {False: 54}, 'cost_price': {False: 54}},\n",
       "   'negative_validity': {'unit_price': 2, 'cost_price': 3}},\n",
       "  'inventory_tracking': {'missing_values': {'tracking_id': 0,\n",
       "    'product_id': 0,\n",
       "    'quantity_change': 0,\n",
       "    'change_date': 0,\n",
       "    'reason': 0,\n",
       "    'created_at': 0},\n",
       "   'date_validity': {'change_date': {'True': 162}},\n",
       "   'numeric_validity': {'quantity_change': {True: 162}},\n",
       "   'negative_validity': {'quantity_change': 0}},\n",
       "  'orders': {'missing_values': {'order_id': 0,\n",
       "    'employee_id': 0,\n",
       "    'customer_id': 250,\n",
       "    'order_date': 0,\n",
       "    'total_amount': 0,\n",
       "    'payment_method': 0,\n",
       "    'order_status': 0,\n",
       "    'created_at': 0},\n",
       "   'date_validity': {'order_date': {'True': 1010}},\n",
       "   'numeric_validity': {'total_amount': {True: 1010}},\n",
       "   'negative_validity': {'total_amount': 0}},\n",
       "  'order_details': {'missing_values': {'order_detail_id': 0,\n",
       "    'order_id': 0,\n",
       "    'product_id': 0,\n",
       "    'quantity': 0,\n",
       "    'unit_price': 0,\n",
       "    'subtotal': 0,\n",
       "    'created_at': 0},\n",
       "   'date_validity': {},\n",
       "   'numeric_validity': {'unit_price': {True: 3022},\n",
       "    'quantity': {True: 3022},\n",
       "    'subtotal': {True: 3022}},\n",
       "   'negative_validity': {'unit_price': 0, 'quantity': 0, 'subtotal': 0}},\n",
       "  'customers': {'missing_values': {'customer_id': 0,\n",
       "    'first_name': 0,\n",
       "    'last_name': 0,\n",
       "    'email': 0,\n",
       "    'phone': 4,\n",
       "    'loyalty_points': 0,\n",
       "    'created_at': 0},\n",
       "   'date_validity': {},\n",
       "   'numeric_validity': {'loyalty_points': {True: 204}},\n",
       "   'negative_validity': {'loyalty_points': 8}},\n",
       "  'employees': {'missing_values': {'employee_id': 0,\n",
       "    'first_name': 0,\n",
       "    'last_name': 0,\n",
       "    'hire_date': 0,\n",
       "    'role': 0,\n",
       "    'email': 0,\n",
       "    'created_at': 0},\n",
       "   'date_validity': {'hire_date': {'True': 103}},\n",
       "   'numeric_validity': {},\n",
       "   'negative_validity': {}}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "DB_SOURCE = {\n",
    "    \"dbname\": os.getenv(\"SRC_POSTGRES_DB\"),\n",
    "    \"user\": os.getenv(\"SRC_POSTGRES_USER\"),\n",
    "    \"password\": os.getenv(\"SRC_POSTGRES_PASSWORD\"),\n",
    "    \"host\": os.getenv(\"SRC_POSTGRES_HOST\"),\n",
    "    \"port\": os.getenv(\"SRC_POSTGRES_PORT\")\n",
    "}\n",
    "\n",
    "def db_engine():\n",
    "    \"\"\"Creates a SQLAlchemy engine.\"\"\"\n",
    "    try:\n",
    "        db_url = f\"postgresql://{DB_SOURCE['user']}:{DB_SOURCE['password']}@{DB_SOURCE['host']}:{DB_SOURCE['port']}/{DB_SOURCE['dbname']}\"\n",
    "        engine = create_engine(db_url)\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating engine: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_missing_values(table_name):\n",
    "    \"\"\"Check for missing values in a table.\"\"\"\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name}\", db_engine())\n",
    "    missing_values = {}\n",
    "    for col in df.columns:\n",
    "        missing_values[col] = int(df[col].isnull().sum())\n",
    "    return missing_values\n",
    "\n",
    "def is_valid_date(date_str, date_format):\n",
    "    \"\"\"Validate if a string matches the specified date format using regex.\"\"\"\n",
    "    # Define regex patterns based on the format\n",
    "    patterns = {\n",
    "        \"%Y-%m-%d\": r\"^\\d{4}-\\d{2}-\\d{2}$\",  # Matches format 2025-02-16\n",
    "        \"%Y-%m-%d %H:%M:%S\": r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$\",  # Matches format 2025-02-16 14:30:00\n",
    "        \"%d/%m/%Y\": r\"^(0?[1-9]|[12][0-9]|3[01])/(0?[1-9]|1[0-2])/\\d{4}$\"  # Matches format 16/02/2025\n",
    "    }\n",
    "\n",
    "    # Check if the regex for the given format exists and matches the date string\n",
    "    if date_format in patterns:\n",
    "        pattern = patterns[date_format]\n",
    "        return bool(re.match(pattern, date_str))  # Returns True if match found, False otherwise\n",
    "    return False  # If no matching pattern, return False\n",
    "\n",
    "def validate_date_format(data, selected_columns):\n",
    "    \"\"\"Check if the date format in a column is valid based on the specified date formats, and summarize counts of True/False.\"\"\"\n",
    "    result = {}\n",
    "    for table, columns in selected_columns.items():\n",
    "        if table in data:\n",
    "            result[table] = {}\n",
    "            for col, date_format in columns.items():\n",
    "                if col in data[table].columns:\n",
    "                    valid_dates = {}  # Dictionary to store only True/False counts that exist\n",
    "                    for idx, date_str in enumerate(data[table][col].astype(str)):  # Loop through each date value in the column\n",
    "                        is_valid = is_valid_date(date_str, date_format)  # Validate each date string\n",
    "                        # Add to the dictionary only if the result is True or False\n",
    "                        if is_valid:\n",
    "                            valid_dates['True'] = valid_dates.get('True', 0) + 1\n",
    "                        else:\n",
    "                            valid_dates['False'] = valid_dates.get('False', 0) + 1\n",
    "                    result[table][col] = valid_dates  # Save the counts for this column\n",
    "    return result\n",
    "    \n",
    "def check_numeric_values(data, selected_columns):\n",
    "    \"\"\"Check if a column contains numeric values (using for loop).\"\"\"\n",
    "    result = {}\n",
    "    for table, columns in selected_columns.items():\n",
    "        if table in data:\n",
    "            result[table] = {}\n",
    "            for col in columns:\n",
    "                if col in data[table].columns:\n",
    "                    df = data[table][col]\n",
    "                    numeric_check = []  # List to store True/False results\n",
    "                    for x in df:  # Iterate through each element in the Series\n",
    "                        numeric_check.append(isinstance(x, (int, float)))\n",
    "\n",
    "                    # Convert the list to a pandas Series for value_counts()\n",
    "                    numeric_series = pd.Series(numeric_check)\n",
    "                    result[table][col] = numeric_series.value_counts().to_dict()\n",
    "    return result\n",
    "\n",
    "def check_negative_values(data, selected_columns):\n",
    "    \"\"\"Check for negative values in numerical columns, considering both actual negative values and the presence of a '-' symbol.\"\"\"\n",
    "    result = {}\n",
    "    for table, columns in selected_columns.items():\n",
    "        if table in data:\n",
    "            result[table] = {}\n",
    "            for col in columns:\n",
    "                if col in data[table].columns:\n",
    "                    # Get the column data\n",
    "                    df = data[table][col]\n",
    "                    \n",
    "                    # Convert to numeric values (invalid entries become NaN)\n",
    "                    numeric_df = pd.to_numeric(df, errors='coerce')\n",
    "                    \n",
    "                    # Count values that are less than 0 (this handles actual negative values)\n",
    "                    negative_values_count = (numeric_df < 0).sum()\n",
    "                    \n",
    "                    # Ensure the column is of string type to use .str.contains\n",
    "                    # Convert the values to strings and count entries with a '-' symbol (handles cases like \"$-4\" or \"-$4\")\n",
    "                    negative_symbol_count = df.astype(str).str.contains('-').sum()\n",
    "                    \n",
    "                    # The final negative count will be the sum of both conditions\n",
    "                    result[table][col] = negative_values_count + negative_symbol_count\n",
    "\n",
    "    return result\n",
    "\n",
    "def convert_to_serializable(value):\n",
    "    \"\"\"Convert pandas types to native Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(value, pd.Timestamp):\n",
    "        return value.isoformat()  # Convert pandas Timestamp to ISO format\n",
    "    if isinstance(value, pd.Timedelta):\n",
    "        return str(value)  # Convert pandas Timedelta to string\n",
    "    if isinstance(value, (pd.Int64Dtype, pd.Float64Dtype)):\n",
    "        return value.item()  # Convert pandas nullable types to native Python types (int or float)\n",
    "    if pd.api.types.is_integer_dtype(value):  # Check if it's an integer type\n",
    "        return int(value)\n",
    "    if pd.api.types.is_float_dtype(value): # Check if it's a float type\n",
    "        return float(value)\n",
    "    if isinstance(value, (int, float)):\n",
    "        return value  # If it's already a native int or float, return as is\n",
    "    return value  # Return other types (like strings) as they are\n",
    "\n",
    "def generate_data_quality_report():\n",
    "    \"\"\"Generate and save a data quality report.\"\"\"\n",
    "    # Extract data\n",
    "    data = extract_data()\n",
    "\n",
    "    # Define selected columns for validation\n",
    "    date_columns = {\n",
    "        \"employees\": {\"hire_date\": \"%Y-%m-%d\"},\n",
    "        \"inventory_tracking\": {\"change_date\": \"%Y-%m-%d\"},\n",
    "        \"orders\": {\"order_date\": \"%Y-%m-%d %H:%M:%S\"}\n",
    "    }\n",
    "    \n",
    "    numeric_columns = {\n",
    "        \"products\": [\"unit_price\", \"cost_price\"],\n",
    "        \"orders\": [\"total_amount\"],\n",
    "        \"order_details\": [\"unit_price\", \"quantity\", \"subtotal\"],\n",
    "        \"inventory_tracking\": [\"quantity_change\"],\n",
    "        \"customers\": [\"loyalty_points\"]\n",
    "    }\n",
    "    \n",
    "    negative_columns = {\n",
    "        \"products\": [\"unit_price\", \"cost_price\"],\n",
    "        \"orders\": [\"total_amount\"],\n",
    "        \"order_details\": [\"unit_price\", \"quantity\", \"subtotal\"],\n",
    "        \"inventory_tracking\": [\"quantity_change\"],\n",
    "        \"customers\": [\"loyalty_points\"]\n",
    "    }\n",
    "\n",
    "    # Initialize report structure\n",
    "    report = {\n",
    "        \"person_in_charge\": \"Reza\",\n",
    "        \"date_quality_check\": str(datetime.now()),\n",
    "        \"result\": {}\n",
    "    }\n",
    "\n",
    "    # Generate the report for each table\n",
    "    for table in data.keys():\n",
    "        table_report = {}\n",
    "\n",
    "        # Check for missing values\n",
    "        missing_values = {}\n",
    "        for col, val in check_missing_values(table).items():\n",
    "            missing_values[col] = convert_to_serializable(val)\n",
    "        table_report[\"missing_values\"] = missing_values\n",
    "\n",
    "        # Validate dates\n",
    "        date_validity = {}\n",
    "        date_validation_results = validate_date_format(data, date_columns).get(table, {}) # Get the dictionary for the current table or empty if not present\n",
    "        for col, val in date_validation_results.items():\n",
    "            date_validity[col] = convert_to_serializable(val)\n",
    "        table_report[\"date_validity\"] = date_validity\n",
    "\n",
    "        # Check numeric values\n",
    "        numeric_validity = {}\n",
    "        numeric_validation_results = check_numeric_values(data, numeric_columns).get(table, {}) # Get the dictionary for the current table or empty if not present\n",
    "        for col, val in numeric_validation_results.items():\n",
    "            numeric_validity[col] = convert_to_serializable(val)\n",
    "        table_report[\"numeric_validity\"] = numeric_validity\n",
    "\n",
    "        # Check for negative values\n",
    "        negative_validity = {}\n",
    "        negative_validation_results = check_negative_values(data, negative_columns).get(table, {}) # Get the dictionary for the current table or empty if not present\n",
    "        for col, val in negative_validation_results.items():\n",
    "            negative_validity[col] = convert_to_serializable(val)\n",
    "        table_report[\"negative_validity\"] = negative_validity\n",
    "        \n",
    "        # Add table report to main report\n",
    "        report[\"result\"][table] = table_report\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    output_folder = 'output'\n",
    "    output_file = 'paccafe_quality_report.json'\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Save the report as JSON in the /output/ folder\n",
    "    output_path = os.path.join(output_folder, output_file)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "\n",
    "    print(f\"Data quality report saved to {output_path}\")\n",
    "\n",
    "    return report\n",
    "\n",
    "def extract_data():\n",
    "    \"\"\"Extract data from all tables into a dictionary of DataFrames.\"\"\"\n",
    "    tables = list_tables()\n",
    "    engine = db_engine()\n",
    "    data = {}\n",
    "    for table in tables:\n",
    "        data[table] = pd.read_sql(f\"SELECT * FROM {table}\", engine)\n",
    "    return data\n",
    "\n",
    "def list_tables():\n",
    "    \"\"\"Retrieve all table names from the database.\"\"\"\n",
    "    engine = db_engine()\n",
    "    query = \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    return df[\"table_name\"].tolist()\n",
    "\n",
    "# Run the data quality check report\n",
    "generate_data_quality_report()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recommendations** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the dataset, the following steps are recommended to improve data quality and ensure consistency for further processing.\n",
    "\n",
    "1.   **Data Type Conversion:** Convert the `unit_price` and `cost_price` columns in the `products` table from string to integer. This aligns with the integer data type of the `unit_price` column in the `order_detail` table and facilitates numerical analysis.\n",
    "\n",
    "2.   **Data Cleaning:** Advise users to remove any currency symbols or special characters from the `unit_price` and `cost_price` values in the `products` table. These characters can interfere with accurate numerical calculations.\n",
    "\n",
    "3.   **Negative Value Handling:** Consult with users regarding the negative values present in the `unit_price` and `cost_price` columns of the `products` table. Determine whether these values should be removed, corrected, or handled in another specific way.\n",
    "\n",
    "4.   **Missing Customer IDs:** Discuss with users the 250 missing `customer_id` values.  Explore options such as replacing them with a `guest_id`, ignoring them, or attempting to complete the missing data if sufficient information is available.\n",
    "\n",
    "5.   **\"ERROR\" Payment Method:** Confirm with users whether the `payment_method = \"ERROR\"` entries in the `orders` table should be retained as is or corrected to the appropriate payment method.\n",
    "\n",
    "6.   **Missing Phone Numbers:** Discuss with users how to handle the missing phone number data in the `customer` table. Options include leaving the data as missing, deleting the corresponding records, or planning to update the data at a later time.\n",
    "\n",
    "7.   **Employee Role Data:** Consult with users about the role `today`, `third`, and `me` columns in the `employees` table.  Decide whether these values should be corrected, kept as they are, or removed entirely.\n",
    "\n",
    "8.   **Date Format Conversion:** Recommend converting the `hire_date` column in the `employees` table from its current object data type to the `datetime` format. This will enable proper date-based analysis and filtering.\n",
    "\n",
    "9.   **Data Quality Metrics and Validation Strategy:** Discuss with users a comprehensive strategy for handling missing values, date validity, numeric validity and negative validity throughout the dataset.  This may involve establishing percentage thresholds (e.g., accepting validation data below 5%, while requiring correction above 5%) or other appropriate methods for imputation or removal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
